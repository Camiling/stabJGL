---
title: "`JoStARS`: penalty parameter selection in joint network inference"
author: "Camilla Lingjaerde"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{`JoStARS`: penalty parameter selection in joint network inference}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 3
)
```

```{r setup, include=FALSE, warning=FALSE}
library(JoStARS)
library(foreach)
```
 

 
> [Introduction](#intro)

> [Overview of package](#overview)

> [The joint graphical lasso](#JGL)

> [JoStARS](#JoStARS)

> [Examples](#ex)



<a id="intro"></a>


# Introduction
 

The `JoStARS` package implements JoStARS for penalty parameter selection in joint network reconstruction of multiple graphs, in the setting of Gaussian graphical models. JoStARS performs penalty parameter selection in the joint graphical lasso of @danaher2014, selecting both the sparsity controlling and the between-graph similarity controlling penalty parameters. The objective is to borrow strength across similar classes to increase statistical power, while ensuring that the joint modelling may not decrease the accuracy of the resulting inferred graphs. The method takes a list of data matrices for which separate graphs are to be inferred, and selects the penalty parameters with a data-driven approach. The sparsity controlling parameter is selected based on graph stability (@liu2010stability), while the similarity controlling parameter is selected with a likelihood-based approach (@foygel2010extended). 

In this vignette, a brief introduction to Gaussian graphical network models, as well as the graphical lasso and its extension into the joint graphical lasso, is given. A description of the methodology of JoStARS is then given, followed by an example of the package usage. 



<a id="overview"></a>

<br>

# Overview of package

## Functions

Here is an overview of the main functions. You can read their documentation and see examples 
with `?function_name`.

----------------------------- ------------------------------------------------------
Function Name                 Description
----------------------------- ------------------------------------------------------
`JoStARS`                     Performs JoStARS.

`adapted_eBIC`                Calculates the adapted extended BIC (adapted eBIC) score of a set of estimated precision matrices.

`sparsity`                    Finds the sparsity of a graph.

`precision`                   Finds the precision of an estimated graph. 

`recall`                      Finds the recall of an estimated graph.

`confusion.matrix`            Finds the confusion matrix between a graph and its estimate.
----------------------------- --------------------------------------------------

: Main functions in the `JoStARS` package.



<a id="JGL"></a>

<br>

# The joint graphical lasso

 
 
##Gaussian graphical network models

Network models are popular tools for modelling interactions and associations. While the edges of correlation networks are indicative of direct associations, correlation is not a neccessary or suffient condition (@chen2018) for a direct association. In a conditional independence network, the effect of other variables (nodes) is adjusted for. In such a graph, there is an edge between two nodes if and only if there is a direct association between them when conditioning upon the rest of the variables in the graph.

Inferring a conditional independence network requires a measure of the conditional dependence between all pairs of nodes or variables. Given that each node is associated with a measurable *node attribute*, one possible approach is to assume a Gaussian graphical model, where the multivariate random vector $(X_1, \ldots, X_{p})^T$ of node attributes is assumed to be multivariate Gaussian with mean vector $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}$. A conditional independence network may then be determined from the inverse covariance matrix, or *precision matrix*, $\boldsymbol{\Theta}=\boldsymbol{\Sigma}^{-1}$. Given the entries $\theta_{ij}$ of $\boldsymbol{\Theta}$, the conditional (or partial) correlation between nodes, or variables, $i$ and $j$ conditioned upon all others is given by 

$$
\rho_{ij\vert V\backslash \{i,j\} } = - \frac{\theta_{ij}}{\sqrt{\theta_{ii}\theta_{jj}}}
$$
where $V$ is the set of all node pairs. Since correlation equal to zero is equivalent to independence for Gaussian variables, a conditional independence graph may be constructed by determining the non-zero entries of the precision matrix $\boldsymbol{\Theta}$ and assigning edges to the corresponding node pairs. The resulting model is a *Gaussian graphical model*, with the edges representing conditional dependence.

Due to the high-dimensional problem frequently encountered in real-life data, where the number of observations $n$ is much smaller than the number of parameters to estimate (in this case elements in $\boldsymbol{\Theta}$), the sample covariance matrix is often not of full rank. Thus, the inverse covariance matrix is often not possible to compute directly, and alternative approaches are neccessary.  


## The graphical lasso

The *sparsity* of a graph is the number of edges divided by the maximal number of edges the graph can have. If the sparsity of a graph is sufficiently small, $\boldsymbol{\Theta}$ has enough zero elements to be of full rank and thus invertible and well defined. We then say that the graph, or precision matrix, is sparse.

The graphical lasso performs sparse precision matrix estimation by imposing an $L_1$ penalty on the matrix entries (@friedman2008). The graphical lasso aims to solve the penalized log-likelihood problem

$$
 \widehat{\boldsymbol{\Theta}} = \text{argmax}_{\boldsymbol{\Theta} \succ 0} \Big \{\log (\det\boldsymbol{\Theta}) - \text{tr} (\boldsymbol{S} \boldsymbol{\Theta}) - \lambda \| \boldsymbol{\Theta }\|_1 \Big \}, 
$$

where  \(\boldsymbol{\Theta} \succ 0\) is the requirement that \(\boldsymbol{\Theta} \) is positive definite, tr denotes the trace and $\|\cdot \|_1$ denotes the $L_1$ norm. Letting $\boldsymbol{X}$ be the $n\times p$ matrix of observed data, with each row corresponding to one of $n$ observations of the multivariate random vector of attributes, $\boldsymbol{S}$ is the sample covariance matrix of the data.  

The graphical lasso is implemented in the R packages `glasso` (@glasso) and `huge` (@huge), with the latter providing several routines for selection of the penalty parameter $\lambda$.



## The joint graphical lasso

If one is interested in the graph structures of several data types, one possibility is to infer separate graphical lasso networks for each data type. However, if the data types are related or have similar underlying graph structures, a joint modelling approach can greatly increase statistical power. One such approach is the joint graphical lasso (JGL) of @danaher2014, in which one borrows strength across the data types to estimate multiple graphical models that share certain characteristics. This results in one network for each data type, where the network inference will improve if the different types of data are informative for each other. 

The joint graphical lasso is a generalized method for jointly estimating Gaussian graphical models on multiple types or sets of data. @danaher2014 use the term *classes* about the different data types for which they want to infer separate networks. The classes correspond to distinct but related conditions or data types. For $K$ classes, we let $\{\boldsymbol{\Theta}\} = (\boldsymbol{\Theta}^{(1)},\ldots, \boldsymbol{\Theta}^{(K)})$ be the set of their precision matrices. It is assumed that the set of $\sum_{k=1}^K n_k$ observations are independent. 

In the joint graphical lasso, a penalty function is applied to the differences between the precision matrix estimates. @danaher2014 propose two different penalty functions, of which they recommend the *fused graphical lasso* for most applications. The fused graphical lasso employs a fused penalty to encourage both similar network structure and similar inverse covariance matrices across classes. The method aims to solve the penalized log-likelihood problem

$$
 \{\widehat{\boldsymbol{\Theta}} \}= \text{argmax}_{\{\boldsymbol{\Theta} \succ 0\}}  \Big \{ \sum\limits_{k=1}^{K} n_k [\log (\det \boldsymbol{\Theta}^{(k)} ) - \text{tr} (\boldsymbol{S}^{(k)}  \boldsymbol{\Theta}^{(k)} )] - \lambda_1\sum\limits_{k=1}^K\sum\limits_{i\neq j}|\theta_{ij}^{(k)}| - \lambda_2 \sum\limits_{k<k'}\| \boldsymbol{\Theta}^{(k)} - \boldsymbol{\Theta}^{(k')}\|_1 \}
$$

where $\boldsymbol{S}^{(k)}$ is the sample covariance matrix of group $k$, $n_k$ the number of observations in the group, $|\theta_{ij}^{(k)}|$ the absolute value of the $ij^{\text{th}}$ element of $\boldsymbol{\Theta}^{(k)}$ and $\lambda_1$ and $\lambda_2$ are positive penalty parameters. 

The fused graphical lasso applies $\text{L}_1$ penalties to each off-diagonal element of the $K$ precision matrices as well as to the differences between corresponding elements of each pair of precision matrices. The method can be modified to penalize the diagonal elements as well. Like in the graphical lasso, the parameter $\lambda_1$ controls the sparsity. The additional parameter $\lambda_2$ controls the degree to which the $K$ precision matrices are forced towards each other, encouraging not only similar network structure but also similar partial covariances. @danaher2014 suggest using an approximation of the AIC to select $\lambda_1$ and $\lambda_2$, suggesting to select the parameters sequentially to avoid a computationally costly grid search. 

The joint graphical lasso is implemented in the R package `JGL` (@JGL), with the fused penalty as the default. 



<a id="JoStARS"></a>

<br>

# JoStARS


<!---For several classes of data with similar underlying graph structures, the joint graphical lasso can drastically improve the accuracy of inferred graphs compared to the ordinary graphical lasso applied to the data from each class separately. The more similar the classes are, the more the graph inference will benefit from sharing information across classes. The penalty parameters $\lambda_1$ and $\lambda_2$ in the joint (fused) graphical lasso do however need to be chosen, and this is not a straightforward task. While the existing penalty parameter selection method based on the adapted AIC of @danaher2014 is computationally fast, the AIC is not a good fit for penalty selection in high-dimensional situations and tends to result in drastic over-selection of edges (@foygel2010extended, @liu2010stability). --->


While the existing penalty parameter selection method for the joint graphical lasso based on the adapted AIC of @danaher2014 is computationally fast, the AIC is not a good fit for sparsity selection in high-dimensional situations and tends to result in drastic over-selection of edges (@foygel2010extended, @liu2010stability). JoStARS performs penalty parameter selection in the joint (fused) graphical lasso of @danaher2014, avoiding the problem of severe over- or under-selection of edges commonly associated with AIC based sparsity selection. The objective is to achieve appropriate sparsity, as well as to borrow strength across similar classes to increase statistical power while ensuring that the joint modelling may not decrease the accuracy of the resulting inferred graphs. 

In JoStARS, the sparsity controlling parameter $\lambda_1$ is selected based on graph stability under subsampling (@liu2010stability), while the similarity controlling parameter $\lambda_2$ is selected with a likelihood-based approach (@foygel2010extended). In other words, JoStARS selects $\lambda_1$ based on graph stability and $\lambda_2$ based on the amount of similarity supported by the data. 


## Selecting $\lambda_1$

JoStARS starts by selecting $\lambda_1$, fixing $\lambda_2$ to a small value such as $0.01$. This way, JoStARS ensures that $\lambda_1$ is selected without $\lambda_2$ encouraging too much similarity, which could influence the selected the value of $\lambda_1$. The sparsity-controlling $\lambda_1$ is selected based on stability of graphs under subsampling, using the same reasoning as in the StARS method for sparsity selection (@liu2010stability). The method aims to select the least amount of regularization that makes graphs sparse as well as reproducible under random sampling. 

In JoStARS, $\lambda_1$ is selected by conducting many subsamplings and estimating the graph *variability*. For each value of $\lambda_1$ to consider, joint graphical lasso graphs are estimated for each set of sampled data, and the variability of each edge within each of the $K$ graph classes is measured. This edge variability is measured by estimating the variance of the binomial indicator of the event that two subsample graphs will agree on the edge. The variability of each graph is then measured as the average edge variability. Finally, the total variability of the set of joint graphical lasso graphs is measured as the average variability across all $K$ classes. This results in a variability estimate for each value of $\lambda_1$ to consider. 

The smallest value of $\lambda_1$ whose resulting variability measure that does not exceed a predefined threshold $\beta_1$ is then selected. A threshold of $\beta_1=0.1$ is proposed, as suggested by (@liu2010stability) in the ordinary StARS algorithm. This threshold has a nice interpretability, as it means that up to $10\%$ instability, i.e. wrongful edge edge assignments in the graph, is accepted.


## Selecting $\lambda_2$


After $\lambda_1$ has been selected, JoStARS selects $\lambda_2$ with an adapted version of the extended BIC of @foygel2010extended. In order to adjust the criterion to be computable for a set of precision matrices, it is modified in a similar fashion to the modified AIC of @danaher2014. The adapted eBIC score is given by


$$
\text{BIC}_{\gamma}(\lambda_1,\lambda_2) = \sum_{k=1}^K \left[ n_k \text{tr}(\boldsymbol{S}^{(k)}\widehat{\boldsymbol{\Theta}}^{(k)}_{\lambda_1,\lambda_2}) - n_k \log(\det\widehat{\boldsymbol{\Theta}}^{(k)}_{\lambda_1,\lambda_2}) + \vert E_{\lambda_1,\lambda_2}^{(k)} \vert \log{n_k} + 4 \vert E_{\lambda_1,\lambda_2}^{(k)}  \vert \gamma \log{p} \right]
$$

where $\widehat{\boldsymbol{\Theta}}^{(k)}_{\lambda_1,\lambda_2}$ is the estimated precision matrix of class $k$ corresponding to the penalty parameters $\lambda_1$ and $\lambda_2$, and $|E_{\lambda_1,\lambda_2}^{(k)}|$ is the size of the corresponding edge set. 

By considering a grid of $\lambda_2$ values while $\lambda_1$ is fixed to the value selected in the previous step, the optimal value of $\lambda_2$ as selected by JoStARS is the one minimizing the adapted eBIC score. Like for the ordinary eBIC, the additional edge penalty parameter $\gamma\in [0,1]$ must be chosen. Notably, as the eBIC is not used for sparsity selection, but similarity selection, the choice of $\gamma$ is not as important since the criterion is being used to compare graphs with the same value of $\lambda_1$ and hence similar sparsity.

## Re-selecting $\lambda_1$

Because of the shrinkage encouraged by the $L_1$ penalty used on the differences between precision matrices in the joint graphical lasso, the choice of $\lambda_2$ will affect the sparsity of the final inferred graph to some extent. JoStARS has already selected an appropriate sparsity level when selecting $\lambda_1$, and a chosen value of $\lambda_2$ that is very different from the initial value it was fixed to in the tuning of $\lambda_1$ could result in a very different sparsity level. If this is a concern, one might want to re-tune $\lambda_1$ by re-running the selection routine for it described above, with $\lambda_2$ fixed to the new value selected by the adapted eBIC. This will however result in a loss of computational efficiency, but should be considered if $\lambda_2$ is selected to be very different from its initial value.

## Summary

For several classes of data with similar underlying graph structures, the joint graphical lasso can drastically improve the accuracy of inferred graphs compared to the ordinary graphical lasso applied to the data from each class separately. The more similar the classes are, the more the graph inference will benefit from sharing information across classes. The penalty parameters $\lambda_1$ and $\lambda_2$ do however need to be chosen, and this should be done both with respect to achieving an accurate sparsity level and with resepect to achieving appropriate similarity. 

By selecting the sparsity-controlling parameter $\lambda_1$ based on within-graph stability under subsamplings, JoStARS avoids the problem of severe over- or under-selection of edges commonly associated with AIC based sparsity selection (@liu2010stability, @foygel2010extended). Since the similarity controlling parameter $\lambda_2$ is chosen with a penalised likelihood-based approach constructed for high-dimensional cases, JoStARS enforces as much similarity as is supported by the data while avoiding a favouring of large graphs. This way, too strong similarity encouragement that might negatively affect the model fit is avoided. In other words, there is much to gain yet little to lose by using JoStARS to select the parameters. 

Additionally, JoStARS also has a convenient interpretability through the selected value of $\lambda_2$, providing a *similarity score* for the classes of data. A chosen value close to zero indicates no similarity while a larger value indicates that encouraging similarity between their graphs improves the inference. 




<!---A large selected value indicates that there is evidence for strong similarity between the graphs, and that encouraging similarity between them will improve the model fit. A chosen value close to zero, on the other hand, indicates that there is no evidence for any similarities between the graphs, and that similarity encouragement is not useful and might in fact negatively affect the model fit. This interpretability is convenient, as the selected $\lambda_2$ can tell how similar the underlying graph structures of the data sets are. --->





<a id="ex"></a>

<br>

# Examples


The main function `JoStARS` takes a list of $K$ data matrices, each of dimension $n_k \times p$ where $n_k$ is the sample size of *class* (or data set) $k$, for which separate graphs are to be inferred. The function uses JoStARS to select the sparsity controlling penalty parameter $\lambda_1$ and similarity controlling penalty parameter $\lambda_2$, and uses the joint graphical lasso (@danaher2014) to estimate the inverse covariance matrices (precision matrix) of each of the $K$ classes. For each class, a graph structure can then be determined by letting there be an edge between all pairs of variables, or nodes, $(i,j)$ whose corresponding element in the estimated precision matrix is non-zero. 

`JoStARS` selects the penalty parameters with a data-driven approach. The sparsity controlling parameter $\lambda_1$ is selected based on within-graph graph stability under subsamplings (@liu2010stability), with the aim of choosing the minimal amount of regularization needed to get sparse graphs that are reproducible under random sampling. This requires many subsamplings and model fittings, and to increase computational efficiency this part of the method can be run in parallel with a user-provided number of threads.

The between-graph similarity controlling parameter $\lambda_2$ is selected with a likelihood-based approach (@foygel2010extended). Using a version of the extended BIC adapted to evaluate a set of graphs as the selection criterion, the method ensures that graph similarity is only enforced to the extent that it improves the model fit. 

Below one simple example with $K=2$ classes (or data sets) that are from the same distribution, and one where they are from completely unrelated distributions, is shown. In the fist case, $\lambda_2$ is selected to be larger than zero, hence encouraging similarity. This results in high precision considering the high dimensionality of the problem. In the latter case, $\lambda_2$ is selected to be exactly zero and no similarity between the two graphs is encouraged. 

In each example, $K=2$ networks as well as corresponding data from a Gaussian graphical distribution are generated using the `huge` R package (@huge). Networks with the *scale-free* property are generated, as it is a known trait in many real-life networks including genomic networks (@kolaczyk09). 





```{r, warning = FALSE}
#  example 1: scale-free data where the data sets are from the same distribution
set.seed(123)
n1 = 100 # let there be different number of samples in each data set
n2 = 80 
p <- 20
dat <- huge::huge.generator(n = n1, d = p, graph = "scale-free")
dat$sparsity # true sparsity level
prec.mat <- dat$omega # the true precision matrix of both data sets
x1 = MASS::mvrnorm(n1, mu=rep(0,p),Sigma=dat$sigma) # data set 1
x2 = MASS::mvrnorm(n2, mu=rep(0,p),Sigma=dat$sigma) # data set 2
Y=list(x1, x2)
res <- JoStARS(Y, lambda2.max=0.3) 
adj.mat1 <- res$opt.fit[[1]] !=0 # the estimated adjacency matrix of graph 1
adj.mat2 <- res$opt.fit[[2]] !=0 # the estimated adjacency matrix of graph 2
res$opt.lambda1 # the optimal selected value of lambda1
res$opt.lambda2 # lambda2 is chosen quite large
res$opt.sparsities # the sparsities of the estimated precision matrices
# Look at precision of inferred graphs
precision(abs(prec.mat) > 1e-7, res$opt.fit[[1]] != 0)
precision(abs(prec.mat) > 1e-7, res$opt.fit[[2]] != 0)

# example 2: scale-free data where where the data sets are from completely unrelated distributions
set.seed(123)
n1 <- 100 
n2 <- 80
p <- 20
dat1 <- huge::huge.generator(n = n1, d = p, graph = "scale-free")
dat2 <- huge::huge.generator(n = n2, d = p, graph = "scale-free") # second graph is completely unrelated
dat1$sparsity # true sparsity level for graph 1
dat2$sparsity # true sparsity level for graph 2
prec.mat1 <- dat1$omega # the true precision matrix of data set 1
prec.mat2 <- dat2$omega # the true precision matrix of data set 2
x1 = MASS::mvrnorm(n1, mu=rep(0,p),Sigma=dat1$sigma)
x2 = MASS::mvrnorm(n2, mu=rep(0,p),Sigma=dat2$sigma)
Y = list(x1, x2)
res <- JoStARS(Y, scale=T,lambda2.max=0.3)
adj.mat1 <- res$opt.fit[[1]] !=0 # the estimated adjacency matrix of graph 1
adj.mat2 <- res$opt.fit[[2]] !=0 # the estimated adjacency matrix of graph 2
res$opt.lambda1 # the optimal selected value of lambda1
res$opt.lambda2 # lambda2 is chosen very small
res$opt.sparsities # the sparsities of the estimated precision matrices
# lower precision as no information could be borrowed across classes
precision(abs(prec.mat1) > 1e-7, adj.mat1 != 0) 
precision(abs(prec.mat2) > 1e-7, adj.mat2 != 0)

```

The resulting JoStARS graphs can be visualised with functions from the `network` and `ggnet2` libraries. 


```{r, results='hide', fig.align='center', out.width='60%'}
set.seed(1234)
net1 =network::network(adj.mat1)
net2 =network::network(adj.mat2)
g1 = GGally::ggnet2(net1,alpha=0.9,color = 'darkorange')
g2 = GGally::ggnet2(net2,alpha=0.9,color = 'darkorange')
ggpubr::ggarrange(g1,g2,ncol=2,nrow=1)
```


## References

 

