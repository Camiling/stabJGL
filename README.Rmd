---
output: rmarkdown::github_document
bibliography: ./inst/REFERENCES.bib
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>",
    fig.path = "man/figures/README-",
    out.width = "100%",
    tidy = "styler"
)
library(stabJGL)
library(foreach)
```


<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![codecov](https://codecov.io/gh/Camiling/stabJGL/branch/main/graph/badge.svg?token=QL5ZW3RQZD)](https://codecov.io/gh/Camiling/stabJGL)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![R-CMD-check](https://github.com/camiling/stabJGL/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/camiling/stabJGL/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

<!-- IF ON CRAN [![CRAN_Status_Badge](https://www.r-pkg.org/badges/version-last-release/shapr)]
[![CRAN_Downloads_Badge](https://cranlogs.r-pkg.org/badges/grand-total/shapr)]---->
<!--PAPER HERE [![DOI](https://joss.theoj.org/papers/10.21105/joss.02027/status.svg)]---->


# `stabJGL` <img src="man/figures/stabJGLlogo.png" align="right" height="150"/>


The `stabJGL` package implements stabJGL for sparsity and similarity selection in joint network reconstruction of multiple graphs, in the setting of Gaussian graphical models. stabJGL performs penalty parameter selection in the joint graphical lasso of @danaher2014, selecting both the sparsity controlling and the between-graph similarity controlling penalty parameters. The objective is to borrow strength across similar classes to increase statistical power, while ensuring that the joint modelling may not decrease the accuracy of the resulting inferred graphs. The method takes a list of data matrices for which separate graphs are to be inferred, and selects the penalty parameters with a data-driven approach. The sparsity controlling parameter is selected based on graph stability (@liu2010stability), while the similarity controlling parameter is selected with a likelihood-based approach (@foygel2010extended). 


## Installation

To install the current development version, use

```{r, eval = FALSE}
remotes::install_github("camiling/stabJGL")
```

If you would like to install all packages of the models we currently support, use

```{r, eval = FALSE}
remotes::install_github("camiling/stabJGL", dependencies = TRUE)
```


If you would also like to build and view the vignette locally, use 

```{r, eval = FALSE}
remotes::install_github("camiling/stabJGL", dependencies = TRUE, build_vignettes = TRUE)
browseVignettes("stabJGL")
```


## Example


The main function `stabJGL` takes a list of $K$ data matrices, each of dimension $n_k \times p$ where $n_k$ is the sample size of data set $k$, for which separate graphs are to be inferred. The function selects the sparsity controlling penalty parameter $\lambda_1$ and similarity controlling penalty parameter $\lambda_2$, and uses the joint graphical lasso (@danaher2014) to estimate the inverse covariance matrices (precision matrix) of each class. 

The following examples show how `stabJGL` selects how strongly similarity should be enforced for a set of $K=2$ graph, as well as determine the sparsity level, and returns the resulting precision matrix estimates. One example with $K=2$ data sets drawn from the same distribution, and one with $K=2$ data sets drawn from completely unrelated distributions, is shown. In the fist case, $\lambda_2$ is selected to be larger than zero, hence encouraging similarity. This results in high precision considering the high dimensionality of the problem. In the latter case, $\lambda_2$ is selected to be exactly zero and no similarity between the two graphs is encouraged. 

The data is generated using the R package `huge` (@huge), as it includes functionality for generating data from a Gaussian graphical model. The networks we generate are *scale-free*, which is a known trait in many real-life networks such as genomic networks (@kolaczyk09).

```{r, warning = FALSE}
#  example 1: scale-free data where the data sets are from the same distribution
set.seed(123)
n1 = 100 # let there be different number of samples in each data set
n2 = 80 
p <- 20
dat <- huge::huge.generator(n = n1, d = p, graph = "scale-free")
dat$sparsity # true sparsity level
prec.mat <- dat$omega # the true precision matrix of both data sets
x1 = MASS::mvrnorm(n1, mu=rep(0,p),Sigma=dat$sigma) # data set 1
x2 = MASS::mvrnorm(n2, mu=rep(0,p),Sigma=dat$sigma) # data set 2
Y=list(x1, x2)
res <- stabJGL(Y, lambda2.max=0.3) 
adj.mat1 <- res$opt.fit[[1]] !=0 # the estimated adjacency matrix of graph 1
adj.mat2 <- res$opt.fit[[2]] !=0 # the estimated adjacency matrix of graph 2
res$opt.lambda1 # the optimal selected value of lambda1
res$opt.lambda2 # lambda2 is chosen quite large
res$opt.sparsities # the sparsities of the estimated precision matrices
# Look at precision of inferred graphs
precision(abs(prec.mat) > 1e-7, res$opt.fit[[1]] != 0)
precision(abs(prec.mat) > 1e-7, res$opt.fit[[2]] != 0)

# example 2: scale-free data where where the data sets are from completely unrelated distributions
set.seed(123)
n1 <- 100 
n2 <- 80
p <- 20
dat1 <- huge::huge.generator(n = n1, d = p, graph = "scale-free")
dat2 <- huge::huge.generator(n = n2, d = p, graph = "scale-free") # second graph is completely unrelated
dat1$sparsity # true sparsity level for graph 1
dat2$sparsity # true sparsity level for graph 2
prec.mat1 <- dat1$omega # the true precision matrix of data set 1
prec.mat2 <- dat2$omega # the true precision matrix of data set 2
x1 = MASS::mvrnorm(n1, mu=rep(0,p),Sigma=dat1$sigma)
x2 = MASS::mvrnorm(n2, mu=rep(0,p),Sigma=dat2$sigma)
Y = list(x1, x2)
res <- stabJGL(Y, scale=T,lambda2.max=0.3)
adj.mat1 <- res$opt.fit[[1]] !=0 # the estimated adjacency matrix of graph 1
adj.mat2 <- res$opt.fit[[2]] !=0 # the estimated adjacency matrix of graph 2
res$opt.lambda1 # the optimal selected value of lambda1
res$opt.lambda2 # lambda2 is chosen very small
res$opt.sparsities # the sparsities of the estimated precision matrices
# lower precision as no information could be borrowed across classes
precision(abs(prec.mat1) > 1e-7, adj.mat1 != 0) 
precision(abs(prec.mat2) > 1e-7, adj.mat2 != 0)

```

The resulting stabJGL graphs can be visualised with functions from the `network` and `ggnet2` libraries. 


```{r,fig.align='center', out.width='60%',results='hide',warning=FALSE}
set.seed(1234)
net1 =network::network(adj.mat1)
net2 =network::network(adj.mat2)
g1 = GGally::ggnet2(net1,alpha=0.9,color = 'darkorange')
g2 = GGally::ggnet2(net2,alpha=0.9,color = 'darkorange')
ggpubr::ggarrange(g1,g2,ncol=2,nrow=1)
```


## Contribution

All feedback and suggestions are very welcome. If you have any questions or comments, feel
free to open an issue [here](https://github.com/Camiling/stabJGL/issues). 



## References
