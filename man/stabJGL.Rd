% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/stabJGL.R
\name{stabJGL}
\alias{stabJGL}
\title{Perform stabJGL}
\usage{
stabJGL(
  Y,
  scale = T,
  penalize.diagonal = FALSE,
  var.thresh = 0.1,
  subsample.ratio = NULL,
  rep.num = 20,
  nlambda1 = 20,
  lambda1.min = 0.01,
  lambda1.max = 1,
  nlambda2 = 20,
  lambda2.min = 0,
  lambda2.max = 0.1,
  lambda2.init = 0.01,
  ebic.gamma = 0.2,
  verbose = T,
  retune.lambda1 = F,
  parallelize = T,
  nCores = 2,
  rho = 1,
  weights = "equal"
)
}
\arguments{
\item{Y}{A list of \eqn{K} data matrices, each of dimension \eqn{n_k} by \eqn{p} where \eqn{n_k} is the sample size of class \eqn{K} and \eqn{p} is the dimension.}

\item{scale}{If \code{scale=TRUE}, all variables will be scaled. Default value is \code{TRUE}.}

\item{penalize.diagonal}{Should the diagonal elements of the precision matrices are to be penalized with \eqn{\lambda_1}? Default value is \code{FALSE}.}

\item{var.thresh}{The variability threshold to use in the selection of \eqn{\lambda_1}. The default value is \eqn{0.1}.}

\item{subsample.ratio}{The subsampling ratio to use when selecting \eqn{\lambda_1}. The default value is \eqn{10*\sqrt(n)/n} when \eqn{n>144} and \eqn{0.8} when \eqn{n\leq 144}, where \eqn{n} is the sample size.}

\item{rep.num}{The number of subsamplings to use when selecting \eqn{\lambda_1}. The default value is \eqn{20}.}

\item{nlambda1}{The number of \eqn{\lambda_1} values to consider. The default value is \eqn{20}.}

\item{lambda1.min}{The smallest value of \eqn{\lambda_1} to consider. The default value is \eqn{0.01}.}

\item{lambda1.max}{The largest value of \eqn{\lambda_1} to consider. The default value is \eqn{0.01}.}

\item{nlambda2}{The number of \eqn{\lambda_2} values to consider. The default value is \eqn{20}.}

\item{lambda2.min}{The smallest value of \eqn{\lambda_2} to consider. The default value is \eqn{0}.}

\item{lambda2.max}{The largest value of \eqn{\lambda_2} to consider. The default value is \eqn{0.1}.}

\item{lambda2.init}{The initial value to fix \eqn{\lambda_2} to when selecting \eqn{\lambda_1}. The default value is \eqn{0.01}.}

\item{ebic.gamma}{The value of \eqn{\gamma} to use in the adapted extended BIC (adapted eBIC) selection criterion. Negative values are not valid. The default value is \eqn{0}.}

\item{verbose}{If \code{verbose = FALSE}, tracing information printing is disabled. The default value is \code{TRUE}.}

\item{retune.lambda1}{Should the sparsity controlling parameter \eqn{\lambda_1} be re-tuned after \eqn{\lambda_2} has been selected? The default value is \code{FALSE}.}

\item{parallelize}{Should the code be parallelized? The default value is \code{TRUE}.}

\item{nCores}{If \code{parallelize=TRUE}, the number of threads to initialize. The default value is 2.}

\item{rho}{A step size parameter to use in the joint graphical lasso. Large values decrease step size. Default value is 1.}

\item{weights}{Determines the putative sample size of each class's data in the joint graphical lasso. Allowed values; a vector with length equal to the number of classes; "\code{equal}", giving each class weight 1; "\code{sample.size}", giving each class weight corresponding to its sample size. The default value is "\code{equal}".}
}
\value{
Object of class \code{"list"}. Contains the following items:
\describe{
\item{opt.fit}{The stabJGL precision matrix estimates. A list of length \eqn{K} precision matrices, each of dimension \eqn{p} by \eqn{p}.}
\item{opt.ebic}{The adapted eBIC value of the set of inferred graphs.}
\item{opt.sparsities}{The sparsities of the inferred graphs. A \eqn{K} dimensional vector.}
\item{opt.lambda1}{The selected value of \eqn{\lambda_1}.}
\item{opt.lambda2}{The selected value of \eqn{\lambda_2}.}
\item{lambda1s}{The sequence of \eqn{\lambda_1} values considered in the selection.}
\item{lambda2s}{The sequence of \eqn{\lambda_2} values considered in the selection.}
\item{ebic.vals}{The adapted eBIC scores of the models corresponding to the different values of \eqn{\lambda_2}.}
\item{opt.fit.lambda1}{The precision matrix estimates found after selecting \eqn{\lambda_1} while \eqn{\lambda_2} is fixed to its initial value. A list of length \eqn{K} precision matrices, each of dimension \eqn{p} by \eqn{p}.}
\item{opt.sparsities.lambda1}{The sparsities of the graphs found after selecting \eqn{\lambda_1} while \eqn{\lambda_2} is fixed to its initial value. A vector of length \eqn{K}.}
\item{total.variability}{The total variability along the subsampling path when selecting \eqn{\lambda_1}. A vector of length \code{nlambda1}.}
\item{variability}{The variability of each class along the subsampling path when selecting \eqn{\lambda_1}. A matrix of dimension \code{nlambda1} by \eqn{K}.}
}
}
\description{
Implements stabJGL for penalty selection in joint network reconstruction of multiple graphs. stabJGL performs penalty parameter selection in the joint graphical lasso, selecting both the sparsity- and the similarity controlling penalty parameters.
}
\details{
The objective is to borrow strength across simialar classes to increase statistical power, while ensuring that the joint modelling may not decrease the accuracy of the resulting inferred graphs. The method takes a set of data matrices for which graphs are to be inferred with the joint graphical lasso. The method takes a list \code{Y} of \eqn{K} data matrices for which separate graphs are to be inferred, selects the sparsity controlling penalty parameter \eqn{\lambda_1} and similarity controlling penalty parameter \eqn{\lambda_2}, and performs the joint graphical lasso, resulting in \eqn{K} precision matrix estimates. To increase computational efficiency, the code can be run in parallel with \code{nCores} threads.
}
\examples{

# example 1: simple example where the data sets are from
#            from completely different distributions
#            and have no edges in their graph structure
# generate data with independent variables
set.seed(1234)
x1 <- matrix(rnorm(5 * 20), ncol = 5)
x2 <- matrix(rnorm(5 * 20), ncol = 5)
Y = list(x1,x2)
# perform stabJGL with variability threshold 0.1
res <- stabJGL(Y)
res$opt.fit # the list of estimated stabJGL precision matrices
res$opt.lambda1 # the optimal selected value of lambda1
res$opt.lambda2 # the optimal selected value of lambda2
res$opt.sparsities # the sparsity of the estimated precision matrices

# example 2: scaling the data
set.seed(123)
res <- stabJGL(Y, scale=TRUE)

# example 3: scale-free data where where the data sets are
#            from identical distributions
set.seed(123)
n <- 80
p <- 100
dat <- huge::huge.generator(n = n, d = p, graph = "scale-free")
x1 = MASS::mvrnorm(n, mu=rep(0,p),Sigma=dat$sigma)
x2 = MASS::mvrnorm(n, mu=rep(0,p),Sigma=dat$sigma)
Y=list(x1, x2)
res <- stabJGL(Y, lambda2.max=0.3)
res$opt.lambda1 # the optimal selected value of lambda1
res$opt.lambda2 # the optimal selected value of lambda2
res$opt.sparsities # the sparsity of the estimated precision matrices
# Look at precision of inferred graphs
precision(abs(dat$omega) > 1e-7, res$opt.fit[[1]] != 0)
precision(abs(dat$omega) > 1e-7, res$opt.fit[[2]] != 0)

# example 4: scale-free data where where the data sets are
#            from completely unrelated distributions
# Create a completely unrelated prior data set
set.seed(123)
n1 <- 80
n2 <- 60
p <- 20
dat1 <- huge::huge.generator(n = n1, d = p, graph = "scale-free")
dat2 <- huge::huge.generator(n = n2, d = p, graph = "scale-free")
x1 = MASS::mvrnorm(n1, mu=rep(0,p),Sigma=dat1$sigma)
x2 = MASS::mvrnorm(n2, mu=rep(0,p),Sigma=dat2$sigma)
Y = list(x1, x2)
res <- stabJGL(Y, scale=TRUE,lambda2.max=0.3)
res$opt.lambda1 # the optimal selected value of lambda1
res$opt.lambda2 # the optimal selected value of lambda2
res$opt.sparsities # the sparsity of the estimated precision matrices
# Look at precision of inferred graphs
precision(abs(dat1$omega) > 1e-7, res$opt.fit[[1]] != 0)
precision(abs(dat2$omega) > 1e-7, res$opt.fit[[2]] != 0)


}
\seealso{
\code{\link[JGL]{JGL}}
}
\author{
Camilla Lingjaerde
}
